using Pkg; Pkg.instantiate()

using GeoStats
using DensityRatioEstimation
using LossFunctions
using ProgressMeter
using DataFrames
using DataDeps
using MLJ, CSV
using Statistics
using Random

# reproducible results
Random.seed!(2020)

# estimators of generalization error
error_cv( m, p, k, â„’) = error(PointwiseLearn(m), p, CrossValidation(k, loss=â„’))
error_bcv(m, p, r, â„’) = error(PointwiseLearn(m), p, BlockCrossValidation(r, loss=â„’))
error_drv(m, p, k, â„’) = error(PointwiseLearn(m), p, DensityRatioValidation(k, loss=â„’, estimator=LSIF(Ïƒ=2.0,b=10)))

# actual error (known labels)
function error_empirical(m, p, â„’)
  t  = task(p)
  Î©s = sourcedata(p)
  Î©t = targetdata(p)

  # learn task on source domain and perform
  # it on both source and target domains
  lm = learn(t, Î©s, m)
  yÌ‚s = perform(t, Î©s, lm)
  yÌ‚t = perform(t, Î©t, lm)

  # error on source
  Ïµs = map(outputvars(t)) do var
    var => LossFunctions.value(â„’[var], Î©s[var], yÌ‚s[var], AggMode.Mean())
  end

  # error on target
  Ïµt = map(outputvars(t)) do var
    var => LossFunctions.value(â„’[var], Î©t[var], yÌ‚t[var], AggMode.Mean())
  end

  Dict(Ïµs), Dict(Ïµt)
end

function experiment(m, p, r, k, â„’)
  # try different error estimates
  cv  = error_cv( m, p, k, â„’)
  bcv = error_bcv(m, p, r, â„’)
  drv = error_drv(m, p, k, â„’)

  # actual error (unhide labels)
  Ïµs, Ïµt = error_empirical(m, p, â„’)

  # model name without suffix
  model = replace(info(m).name, r"(.*)(Regressor|Classifier)" => s"\g<1>")

  map(outputvars(task(p))) do var
    (MODEL=model, SOURCE=Ïµs[var], TARGET=Ïµt[var],
     CV=cv[var], BCV=bcv[var], DRV=drv[var])
  end
end

# -------------
# MAIN SCRIPT
# -------------

# download dataset if needed
register(DataDep("NewZealand",
         "Taranaki Basin Curated Well Logs",
         "https://zenodo.org/record/3832955/files/taranaki-basin-curated-well-logs.tar.gz",
         "608f7aad5a4e9fded6441fd44f242382544d3f61790446175f5ede83f15f4d11",
         post_fetch_method=DataDeps.unpack))

# name of the CSV file in the dataset
csv = joinpath(datadep"NewZealand","taranaki-basin-curated-well-logs","logs.csv")

# logs used in the experiment
logs = [:GR,:SP,:DENS,:NEUT,:DTC]

# read/clean raw data
df = CSV.read(csv)
df = df[:,[logs...,:X,:Y,:Z,:FORMATION,:ONSHORE]]
dropmissing!(df)
categorical!(df, :FORMATION)
categorical!(df, :ONSHORE)
for log in logs
  x = df[!,log]
  Î¼ = mean(x)
  Ïƒ = std(x, mean=Î¼)
  df[!,log] .= (x .- Î¼) ./ Ïƒ
end

# define spatial data
wells = GeoDataFrame(df, [:X,:Y,:Z])

# select the two most frequent formations
formations = groupby(wells, :FORMATION)
frequency = sortperm(npoints.(formations), rev=true)
ð’ž = DataCollection(formations[frequency[1:2]])

# eliminate duplicate coordinates
Î© = uniquecoords(ð’ž)

# split onshore (True) vs. offshore (False)
onoff = groupby(Î©, :ONSHORE)
order = sortperm(onoff[:values], rev=true)
Î©s, Î©t = onoff[order]

# we are left with two formations onshore and offshore
# make sure that these two are balanced for classification
fs, ft = Î©s[:FORMATION], Î©t[:FORMATION]

# formation counts
ms = count(isequal("Urenui"), fs)
ns = count(isequal("Manganui"), fs)
mt = count(isequal("Urenui"), ft)
nt = count(isequal("Manganui"), ft)

# formation proportions
ps = ms / (ms + ns)
pt = mt / (mt + nt)

# weighted sampling
ws = [f == "Urenui" ? 0.5/ps : 0.5/(1-ps) for f in fs]
wt = [f == "Urenui" ? 0.5/pt : 0.5/(1-pt) for f in ft]
Î©s = sample(Î©s, 300000, ws, replace=false)
Î©t = sample(Î©t,  50000, wt, replace=false)

# drop levels to avoid known downstream issues in MLJ
fs, ft = Î©s[:FORMATION], Î©t[:FORMATION]
levels!(fs, ["Urenui","Manganui"])
levels!(ft, ["Urenui","Manganui"])
ð’«s = georef(OrderedDict(:FORMATION => fs), domain(Î©s))
ð’«t = georef(OrderedDict(:FORMATION => ft), domain(Î©t))
Î©s = join(view(Î©s, logs), ð’«s)
Î©t = join(view(Î©t, logs), ð’«t)

# additional configuration without shift
ð’ž = DataCollection(Î©s, Î©t)
fraction = npoints(Î©s) / (npoints(Î©s) + npoints(Î©t))
Î“s, Î“t = split(ð’ž, fraction)

# -------------------
# PROBLEM DEFINITION
# -------------------
# predict formation from well logs
t = ClassificationTask(logs, :FORMATION)

# onshore -> offshore problem
p = LearningProblem(Î©s, Î©t, t)

# problem without shift
q = LearningProblem(Î“s, Î“t, t)

# -----------
# EXPERIMENT
# -----------
@load DummyClassifier pkg="ScikitLearn"
@load RidgeClassifier pkg="ScikitLearn"
@load LogisticClassifier pkg="ScikitLearn"
@load KNeighborsClassifier pkg="ScikitLearn"
@load GaussianNBClassifier pkg="ScikitLearn"
@load BayesianLDA pkg="ScikitLearn"
@load PerceptronClassifier pkg="ScikitLearn"
@load DecisionTreeClassifier pkg="DecisionTree"

# list of models
mrange = [RidgeClassifier(), LogisticClassifier(), KNeighborsClassifier(),
          GaussianNBClassifier(), BayesianLDA(), PerceptronClassifier(),
          DecisionTreeClassifier(), DummyClassifier()]

# block sides and number of folds for error estimators
r = (10000., 10000., 500.)
k = length(GeoStats.partition(Î©s, BlockPartitioner(r)))

# misclassification loss
â„’ = Dict(:FORMATION => MisclassLoss())

# experiment iterator and progress
iterator  = Iterators.product(mrange)
pprogress = Progress(length(iterator), "ONSHORE â†’ OFFSHORE ")
qprogress = Progress(length(iterator), "NO COVARIATE SHIFT ")

# return missing in case of failure
skip = e -> (println("Skipped: $e"); missing)

# perform experiments
presults = progress_pmap(iterator, progress=pprogress, on_error=skip) do (m,)
  experiment(m, p, r, k, â„’)
end
qresults = progress_pmap(iterator, progress=qprogress, on_error=skip) do (m,)
  experiment(m, q, r, k, â„’)
end

# merge all results into a single table
pres = Iterators.flatten(skipmissing(presults))
qres = Iterators.flatten(skipmissing(qresults))
pres = [(SHIFT="YES", r...) for r in pres]
qres = [(SHIFT="NO",  r...) for r in qres]
ares = DataFrame(vcat(pres, qres))

# save all results to disk
fname = joinpath(@__DIR__,"results","newzealand.csv")
CSV.write(fname, ares)
